{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "SKyUhOhESnCn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#https://github.com/YuxinWenRick/hard-prompts-made-easy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install bpemb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POQkZH3bYGmw",
        "outputId": "6c524194-f6bd-4e9d-b6f6-0410335ca869"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bpemb in /usr/local/lib/python3.9/dist-packages (0.3.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from bpemb) (4.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from bpemb) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from bpemb) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from bpemb) (2.27.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from bpemb) (0.1.97)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim->bpemb) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim->bpemb) (6.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n"
      ],
      "metadata": {
        "id": "iIR6V7jAYJO4"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import clip\n",
        "import torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYM47mCeYJi_",
        "outputId": "44afaa11-c71d-4651-bb8b-9a593674121c"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-hpu0pisi\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-hpu0pisi\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (0.14.1+cu116)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "sGvTBbzJYQF7"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CLIP\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "07Ih3KfhYjtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = clip.tokenize\n",
        "token_embedding = clip_model.token_embedding"
      ],
      "metadata": {
        "id": "W6yI8Z68YRob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "transform = T.ToPILImage()"
      ],
      "metadata": {
        "id": "F0q6scfmZ_Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpers"
      ],
      "metadata": {
        "id": "-4uCcLvDh__8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(a, b, c):\n",
        "    a = 1 - torch.mean(a)\n",
        "    b = 1 - torch.mean(b)\n",
        "    c = 1 - torch.mean(c)\n",
        "  \n",
        "    loss = (a + b - c)\n",
        "\n",
        "    return loss\n",
        "     "
      ],
      "metadata": {
        "id": "fMehva_wYS-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_simalirities_images(text, images):\n",
        "    cos_sim_1 = 0\n",
        "    for image in images:\n",
        "      cos_sim_1 += torch.nn.functional.cosine_similarity(text.unsqueeze(1), image.unsqueeze(0), dim=-1)\n",
        "\n",
        "    return cos_sim_1/len(images)"
      ],
      "metadata": {
        "id": "WeXPpBUcYcTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and encode images"
      ],
      "metadata": {
        "id": "lMbHHEG0iFTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "text_1 = \"a bird with a on a branch shovel\"\n",
        "image_1_original = []\n",
        "image_2_original = []\n",
        "count = 0\n",
        "for filename in os.listdir(\"/content/drive/MyDrive/CUB_200_2011/CUB_200_2011/images/165.Chestnut_sided_Warbler/\"):\n",
        "  if count == 10:\n",
        "    break\n",
        "  image_1_original.append(Image.open(\"/content/drive/MyDrive/CUB_200_2011/CUB_200_2011/images/165.Chestnut_sided_Warbler/{}\".format(filename)))\n",
        "  count+=1\n",
        "\n",
        "\n",
        "count = 0\n",
        "text_2 = \"a bird with a on a branch brick\"\n",
        "for filename in os.listdir(\"/content/drive/MyDrive/Birds/CUB_200_2011/images/061.Heermann_Gull/\"):\n",
        "  if count == 10:\n",
        "    break\n",
        "  image_2_original.append(Image.open(\"/content/drive/MyDrive/Birds/CUB_200_2011/images/061.Heermann_Gull/{}\".format(filename)))\n",
        "  count+=1\n"
      ],
      "metadata": {
        "id": "qgYPkSEtYhgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_encode_text(clip_model, text):\n",
        "    x = clip_model.token_embedding(text).type(clip_model.dtype)  # [batch_size, n_ctx, d_model]\n",
        "    #'''\n",
        "    x = x + clip_model.positional_embedding.type(clip_model.dtype)\n",
        "    x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "    x = clip_model.transformer(x)\n",
        "    x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "    x = clip_model.ln_final(x).type(clip_model.dtype)\n",
        "\n",
        "    # x.shape = [batch_size, n_ctx, transformer.width]\n",
        "    # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "    #x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ clip_model.text_projection\n",
        "\n",
        "    x = x @ clip_model.text_projection\n",
        "    # X is now [1,77,512] instead of [1,512]\n",
        "    return x"
      ],
      "metadata": {
        "id": "Z6TTZEq8YnXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize text\n",
        "text_1 = tokenizer([text_1],truncate=True).to(device)\n",
        "\n",
        "text_2 = tokenizer([text_2],truncate=True).to(device)"
      ],
      "metadata": {
        "id": "tRvvWlSwZtzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess images\n",
        "image_1 = []\n",
        "image_2 = []\n",
        "for image in image_1_original:\n",
        "  image_1.append(preprocess(image).unsqueeze(0).to(device))\n",
        "\n",
        "for image in image_2_original:\n",
        "  image_2.append(preprocess(image).unsqueeze(0).to(device))"
      ],
      "metadata": {
        "id": "E3nT6tCBZxyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_grad_enabled(True)\n",
        "\n",
        "# Encode text 1\n",
        "text_1_embed = custom_encode_text(clip_model, text_1).detach()\n",
        "text_1_embed.requires_grad_()\n",
        "\n",
        "# Encode text 2\n",
        "text_2_embed = custom_encode_text(clip_model, text_2).detach()\n",
        "text_2_embed.requires_grad_()\n",
        "\n",
        "\n",
        "# Encode all image 1\n",
        "image_1_embed = []\n",
        "for image in image_1: \n",
        "  image_embed = clip_model.encode_image(image).detach()\n",
        "  image_1_embed.append(image_embed)\n",
        "\n",
        "# Encode all image 2\n",
        "image_2_embed = []\n",
        "for image in image_2: \n",
        "  image_embed = clip_model.encode_image(image).detach()\n",
        "  image_2_embed.append(image_embed)"
      ],
      "metadata": {
        "id": "5jWJ2-MuZ8Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_1_embed_dim =  torch.tensor(text_1_embed[0,8]).clone().requires_grad_()\n",
        "text_2_embed_dim =  torch.tensor(text_2_embed[0,8]).clone().requires_grad_()"
      ],
      "metadata": {
        "id": "IEilEb-G98ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_1_embed.size()"
      ],
      "metadata": {
        "id": "DbZ569elBCHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize optimizer\n",
        "optimizer = torch.optim.SGD([text_1_embed_dim, text_2_embed_dim], lr=0.5)\n",
        "\n",
        "sim_1 = []\n",
        "sim_2 = []\n",
        "sim_3 = []\n",
        "losses = []\n",
        "\n",
        "# Iterations\n",
        "num_epochs = 200\n",
        "\n",
        "text_1_embed = torch.tensor(text_1_embed - text_1_embed_dim).clone().requires_grad_(True)\n",
        "text_2_embed = torch.tensor(text_2_embed - text_2_embed_dim).clone().requires_grad_(True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  text_1_embed = torch.tensor(text_1_embed + text_1_embed_dim).clone().detach().requires_grad_(True)\n",
        "  text_2_embed = torch.tensor(text_2_embed + text_2_embed_dim).clone().detach().requires_grad_(True)\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Compute similarities\n",
        "  cos_sim_1 = compute_simalirities_images(text_1_embed, image_1_embed)\n",
        "  cos_sim_2 = compute_simalirities_images(text_2_embed, image_2_embed)\n",
        "  cos_sim_3 = torch.nn.functional.cosine_similarity(text_1_embed.unsqueeze(1), text_2_embed.unsqueeze(0), dim=-1)\n",
        "  cos_sim_4 = compute_simalirities_images(text_1_embed, image_2_embed)\n",
        "  cos_sim_5 = compute_simalirities_images(text_2_embed, image_1_embed)\n",
        "\n",
        "  # Compute loss\n",
        "  loss = compute_loss(cos_sim_1, cos_sim_2, cos_sim_3)\n",
        "  # Backpropagation\n",
        "  loss.backward()\n",
        "  if epoch % 100 == 0: \n",
        "    print(\"The loss is for epoch {}: {}\".format(epoch, loss))\n",
        "    print(\"The similarity between text 1 and image 1 is: {}\".format(torch.mean(cos_sim_1[0][0])))\n",
        "    print(\"The similarity between text 2 and image 2 is: {}\".format(torch.mean(cos_sim_2[0][0])))\n",
        "    print(\"The similarity between text 1 and 2 is: {}\".format(torch.mean(cos_sim_3[0][0])))\n",
        "    #print(\"The similarity between text 1 and image 2 is: {}\".format(cos_sim_4.item()))\n",
        "    #print(\"The similarity between text 2 and image 1 is: {}\".format(cos_sim_5.item()))\n",
        "\n",
        "\n",
        "  sim_1.append(torch.mean(cos_sim_1[0][0].cpu().detach()))\n",
        "  sim_2.append(torch.mean(cos_sim_2[0][0].cpu().detach()))\n",
        "  sim_3.append(torch.mean(cos_sim_3[0][0].cpu().detach()))\n",
        "  losses.append(torch.mean(loss.cpu().detach()))\n",
        "\n",
        "  # Optimizer step\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "v_J3hfCGaH9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2,2, figsize = (10,10))\n",
        "epochs = np.arange(num_epochs)\n",
        "\n",
        "axes[0][0].plot(epochs, losses)\n",
        "axes[0][0].set_ylabel(\"Loss\")\n",
        "axes[0][0].set_xlabel(\"Epochs\")\n",
        "axes[0][0].set_title(\"Loss over the epochs\")\n",
        "\n",
        "axes[1][0].plot(epochs, sim_1)\n",
        "axes[1][0].set_ylabel(\"Similarity\")\n",
        "axes[1][0].set_xlabel(\"Epochs\")\n",
        "axes[1][0].set_title(\"Similarity image/text 1 \")\n",
        "\n",
        "axes[0][1].plot(epochs, sim_2)\n",
        "axes[0][1].set_ylabel(\"Similarity\")\n",
        "axes[0][1].set_xlabel(\"Epochs\")\n",
        "axes[0][1].set_title(\"Similarity image/text 2 \")\n",
        "\n",
        "axes[1][1].plot(epochs, sim_3)\n",
        "axes[1][1].set_ylabel(\"Similarity\")\n",
        "axes[1][1].set_xlabel(\"Epochs\")\n",
        "axes[1][1].set_title(\"Similarity text 1/text 2\")"
      ],
      "metadata": {
        "id": "BivwvQtIhjZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j4H3_RPAhyo6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}